# 第18章 SQL解析和包`simpledb.parse`

JDBC客户端将SQL语句作为字符串提交给服务器，数据库系统必须处理此字符串以创建可执行的scan。 这个过程分为两个阶段：
- 基于语法的阶段，我们称为`解析(parsing)`阶段； 
- 基于语义的阶段，即`计划(planning)`阶段。 

译者注：我认为planning翻译成计划不是很恰当，下文中直接用planning替代。

我们将在本章讨论解析过程，而在下一章讨论planning。

## 18.1 语法 V.S. 语义
> 语言的`语法(syntax)`是一组描述字符串的规则，这些字符串可能是有意义的语句。

例如，考虑下面这样一个字符串：
```
select 
from table T1 and T2 
where b-3
```
很明显，这肯定是一个非法的SQL语句，因为它不满足SQL的语法，原因是：
- select语句必须包含具体的东西；
- 在from语句中指定的表名前面并不需要加关键字`table`;
- 关键字`and`应该只是在谓词之间起连接作用，而不是在连接表的时候使用；
- 字符串`b-3`也不满足谓词的定义语法。

以上的这么多原因都充分地说明了这个SQL字符串注定不会是一个有意义的SQL语句，数据库也肯定没办法怎么处理这种“SQL”,就算你指定的T1，T2和b真的是表名和字段名。

> 语言的`语义(semantics)`指定了语法正确的字符串的实际含义。

现在，我们考虑一个语法正确的SQL语句，如下：
```
select a
from x,z
where b = 3
```
我们可以推断出，这个SQL语句是一个查询，其功能是从两张表的product结果中返回满足谓词b=3的所有a字段的记录，这个SQL语句因此是有意义的。

一条语句是否有具体还取决定于x，z，a和b的具体语义信息。其中，x和z必须是存在的表名，并且这两张表中至少有一个表存在字段a，并且存在一个int类型的字段b，这些语义信息倒是可以从数据库的元数据中获取到，解析器`parser`一点儿也不知道关于元数据的信息，因此也根本不知道也无法去估计一个SQL语句的具体含义；相反，检查具体元数据的工作是planner的责任，我们将会在第19章中讨论planner。

## 18.2 词法分析
parser为了解析一条SQL语句，首先要做的事情就是把输入的SQL字符串分割成一块一块的`字符(token)`，注意，这里说的字符并不是指单个ASCII码的字符，而是一个单词，例如 I love computer science中包含4个token，即I、love、computer和science。parser中负责将字符串分成token流的部分被称为`词法分析器(lexical analyzer)`。

每个token都有一个类型(type)和一个值(value), SimpleDB中的词法分析器支持5种类型的token，分别为：
- 单字符分隔符，例如，逗号;
- 整形常量，例如123;
- 字符串型常量，例如“joe”;
- 关键字，例如select、from、where等;
- 标识符，例如STUDENT,x, glop34a等。

空格字符（包括空格，制表符和换行符）通常不属于token；唯一的例外是在字符串常量内部的空格字符。空格的作用是用于增强SQL的可读性并使token之间彼此分离。

继续考虑之前的那个例子：
```
select a
from x,z
where b = 3
```
词法分析器会得到下面的10个token，如图18-1所示：

![](ch18/18-1.png)

当parser需要知道token流的适合，会调用词法分析器中的相应方法来得到token。词法分析器中可供parser调用的方法包含两类：
- 第一类是判断当前token是何种类型的方法；
- 第二类是获得当前token的值，并且移动到下一个token的方法，你可以看作好像是词法分析器在不断地“吃”token。

每种类型的token都会在词法分析器中有一对相应的方法，SimpleDB中的词法分析器的API如下所示，包含10个方法：
```Java
public class Lexer {
    public boolean matchDelim(char ch);
    public boolean matchIntConstant();
    public boolean matchStringConstant();
    public boolean matchKeyword(String w);
    public boolean matchIdentifier();

    public void eatDelim(char ch);
    public int eatIntConstant();
    public String eatStringConstant();
    public void eatKeyword(String w);
    public String eatIdentifier();
}
```
前5个方法返回的是当前token的信息，如果当前token和指定的分隔符相等，则方法`matchDelim()`会返回true；类似地，如果当前token和指定的关键字相等，则方法`matchKeyword()`会返回true；其他3个方法则会在当前token满足相关类型时返回true。

后5个方法则会不断地“吃掉”当前token，并移动到下一个token（如果有的话）。每个方法中会调用相应的`matchXXX()`方法，如果match方法返回false，于是抛出一个异常；否则的话，下一个token变长当前的token；此外，`eatIntConstant()`、`eatStringConstant()`和`eatIdentifier()`这3个方法还会返回当前token的具体取值。

## 18.3 实现词法分析器
概念上来说，词法分析器的实现倒是可以很简单粗暴——每次从字符串中读一个字符（注意，这里说的字符真的是一个字符），直到遇到了token之间的分隔符（也就是空格了）则停止，然后判断这个token是什么类型，具体token取值又是什么。因此，词法分析器的复杂度实际上是直接正比于token类型的数量的，token类型越多，这个实现越复杂。（译者注，举个例子，考虑一个SQL字符串`select a from student`，词法分析器一直读读读，'s','e','l','e','c','t',噢，到了一个空格，那前面的就是一个token了，再判断一下，哦这是select，于是知道这是一个关键字token；后续分析类似，因此试想一下，如果token的类型很多，那就会有很多的if判断，真是噩梦）。

好在Java语言中提供了两种不同的自带的`token解析器(tokenizer)`，tokenizer这是Java中的术语，无关紧要了，反正知道是用来解析token的就好了。这两种实现分别是`StringTokenizer`和`StreamTokenizer`。
- `StringTokenizer`用法很简单，但是它只支持两种类型的token：分隔符和单词(也就是两个分隔符之前的内容),稍加分析我们便可以知道，这个实际上不适合SQL，因为这个tokenizer到时候会不理解数值和带引号的字符串；
- 相反，`StreamTokenizer`则支持广泛的token类型，包括SimpleDB中的5种token类型，因此SimpleDB中的`Lexer`类基于Java的stream tokenizer来实现，具体代码如下所示：
```Java
public class Lexer {
    private Collection<String> keywords;
    private StreamTokenizer tokenizer;

    public Lexer(String s) {
        // 初始化关键字
        initKeywords();
        tokenizer = new StreamTokenizer(new StringReader(s));
        // 把'.'也视为一个字符，主要是为了到时候SQL中形如 stuTable.name的字段时，
        // 识别成stuTable,','和name三个token，而不是视stuTable.name为一整个token
        tokenizer.ordinaryChar('.');
        // 使得关键字和标识符大小写不敏感
        tokenizer.lowerCaseMode(true);
        nextToken();
    }

    // =============判断token类型的相关方法==============

    /**
     * 判断当前token是否是指定的分隔符。
     * <p>
     * 在StreamTokenizer中，单字符token的ttype就是字符对应的ASCII码。
     *
     * @param ch
     * @return
     */
    public boolean matchDelim(char ch) {
        return ch == (char) tokenizer.ttype;
    }

    public boolean matchIntConstant() {
        return tokenizer.ttype == StreamTokenizer.TT_NUMBER;
    }

    /**
     * 匹配String的单引号。
     * <p>
     * 注意，在SimpleDB中，字符串常量用单引号包围。
     *
     * @return
     */
    public boolean matchStringConstant() {
        return '\'' == (char) tokenizer.ttype;
    }

    public boolean matchKeyword(String w) {
        return tokenizer.ttype == StreamTokenizer.TT_WORD &&
                tokenizer.sval.equals(w);
    }

    /**
     * 判断是否是标识符
     * <p>
     * 除了关键字以外的word都视为标识符。
     *
     * @return
     */
    public boolean matchIdentifier() {
        return tokenizer.ttype == StreamTokenizer.TT_WORD &&
                !keywords.contains(tokenizer.sval);
    }

    // =============词法分析器不断“吃掉”当前token的相关方法==============
    public void eatDelim(char ch) {
        if (!matchDelim(ch))
            throw new BadSyntaxException();
        nextToken();
    }

    public int eatIntConstant() {
        if (!matchIntConstant())
            throw new BadSyntaxException();
        int i = (int) tokenizer.nval;
        nextToken();
        return i;
    }

    public String eatStringConstant() {
        if (!matchIntConstant())
            throw new BadSyntaxException();
        String str = tokenizer.sval;
        nextToken();
        return str;
    }

    public void eatKeyword(String w) {
        if (!matchKeyword(w))
            throw new BadSyntaxException();
        nextToken();
    }

    public String eatIdentifier() {
        if (!matchIdentifier())
            throw new BadSyntaxException();
        String str = tokenizer.sval;
        nextToken();
        return str;
    }

    /**
     * 得到下一个token
     */
    private void nextToken() {
        try {
            tokenizer.nextToken();
        } catch (IOException e) {
            throw new BadSyntaxException();
        }
    }

    private void initKeywords() {
        keywords = Arrays.asList("select", "from", "where", "and",
                "insert", "into", "values", "delete",
                "update", "set", "create", "table",
                "varchar", "int", "view", "as", "index", "on");
    }
}
```
`Lexer`类的构造函数会创建一个stream tokenizer，其中对`tokenizer.ordinaryChar()`方法的调用作用是告诉tokenize 把'.'也看做是一个token，这样是为了到时候处理SQL中形如 stuTable.name的字段时，识别成stuTable,','和name三个token，而不是视stuTable.name为一整个token（虽然说，在SimpleDB中，我们实际并不会用到点这个符号，但是在真正的SQL语句中必须把这个考虑到）；而方法调用`tokenizer.lowerCaseMode(true)`则会告诉tokenizer将字符串中所有的token全部转为小写的（但是被引号括起来的字符串不会），这样一来，SQL对于关键字和标识符大小写不敏感了。

在StreamTokenizer类中维护了一个public的实例变量，名为`ttype`，表示的是当前token的类型；词法分析器简单地通过这个实例变量的值来判断token的类型，有一个看上去不太好理解的方法就是`matchStringConstant()`，该方法判断的是一个字符串前的单引号字符。

这个只要你稍微看下StreamTokenizer类的文档你就知道是什么意思了，为了方便大家看，我只截出了关于ttype的文档描述，如下：
```java
/**
 * After a call to the {@code nextToken} method, this field
 * contains the type of the token just read. For a single character
 * token, its value is the single character, converted to an integer.
 * For a quoted string token, its value is the quote character.
 * Otherwise, its value is one of the following:
 * <ul>
 * <li>{@code TT_WORD} indicates that the token is a word.
 * <li>{@code TT_NUMBER} indicates that the token is a number.
 * <li>{@code TT_EOL} indicates that the end of line has been read.
 *     The field can only have this value if the
 *     {@code eolIsSignificant} method has been called with the
 *     argument {@code true}.
 * <li>{@code TT_EOF} indicates that the end of the input stream
 *     has been reached.
 * </ul>
 * <p>
 * The initial value of this field is -4.
 *
 * @see     java.io.StreamTokenizer#eolIsSignificant(boolean)
 * @see     java.io.StreamTokenizer#nextToken()
 * @see     java.io.StreamTokenizer#quoteChar(int)
 * @see     java.io.StreamTokenizer#TT_EOF
 * @see     java.io.StreamTokenizer#TT_EOL
 * @see     java.io.StreamTokenizer#TT_NUMBER
 * @see     java.io.StreamTokenizer#TT_WORD
 */
public int ttype = TT_NOTHING;
```
StreamTokenizer类中的`.nextToken()`方法可能会排除一个IOException,我们的`Lexer`类中的私有方法`nextToken()`其实就是对上述方法的一个调用，并且把这个IOException转化为一个BadSyntaxException，到时候这个BadSyntaxException可以传给客户端代码(最终又会边成一个SQLException,我们将会在第20章中看到)。

方法`initKeywords()`则就是完成了初始化SimpleDB中SQL关键字的工作。

